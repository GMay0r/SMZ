{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd120cd3",
   "metadata": {},
   "source": [
    "### Разработка нейросетевых функций. Операция Convolution Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83dedcd",
   "metadata": {},
   "source": [
    "#### Описание параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fda6c5",
   "metadata": {},
   "source": [
    "in_channels (int) – Number of channels in the input image\\\n",
    "out_channels (int) – Number of channels produced by the convolution\\\n",
    "kernel_size (int or tuple) – Size of the convolving kernel\\\n",
    "stride (int or tuple, optional) – Stride of the convolution. Default: 1\\\n",
    "padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\\\n",
    "padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\\\n",
    "dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\\\n",
    "groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\\\n",
    "bias (bool, optional) – If True, adds a learnable bias to the output. Default: True\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def myConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
    "    def Conv(matrix):\n",
    "        if bias == True:\n",
    "            bias_val = torch.rand(out_channels)\n",
    "        else:\n",
    "            bias_val = torch.zeros(out_channels)\n",
    "\n",
    "        if (padding_mode != 'zeros'):\n",
    "            raise Exception('Ошибка. padding_mode задан неверно!')\n",
    "\n",
    "        if type(kernel_size) == tuple:\n",
    "            filter = torch.rand(in_channels, out_channels, kernel_size[0], kernel_size[1])\n",
    "        if type(kernel_size) == int:\n",
    "            filter = torch.rand(in_channels, out_channels, kernel_size, kernel_size)\n",
    "\n",
    "        li = []\n",
    "\n",
    "        for l in range(out_channels):\n",
    "            feature_map = torch.zeros((matrix.shape[1]-1)*stride + dilation * (kernel_size-1)+1, (matrix.shape[2]-1)\n",
    "                                      *stride  + dilation * (kernel_size-1)+1 )\n",
    "            for c in range (in_channels):\n",
    "                for i in range (0, matrix.shape[1]):  \n",
    "                    for j in range (0, matrix.shape[2]):\n",
    "                        val = matrix[c][i][j]\n",
    "                        proizv = val*filter[c][l]\n",
    "                        zero_tensor = torch.zeros((filter.shape[2]-1)*dilation+1, (filter.shape[3]-1)*dilation+1)\n",
    "\n",
    "                        for a in range (0, zero_tensor.shape[0], dilation):\n",
    "                            for b in range (0, zero_tensor.shape[1], dilation):\n",
    "                                zero_tensor[a][b] = proizv[a//dilation][b//dilation]\n",
    "\n",
    "                        res = np.add((zero_tensor), feature_map[i*stride:i*stride+(filter.shape[2]-1)*\n",
    "                                                                dilation+1, j*stride:j*stride+(filter.shape[3]-1)*dilation+1])\n",
    "                        feature_map[i*stride:i*stride+(filter.shape[2]-1)*dilation+1, j*stride:j*stride\n",
    "                                    +(filter.shape[3]-1)*dilation+1] = res\n",
    "\n",
    "\n",
    "            li.append(np.add(feature_map, np.full((feature_map.shape), bias_val[l])))\n",
    "\n",
    "\n",
    "        for t in range(len(li)):\n",
    "            if output_padding > 0:\n",
    "                pad_func = torch.nn.ConstantPad1d((0, output_padding, 0, output_padding), 0)\n",
    "                li[t] = pad_func(li[t])\n",
    "\n",
    "            li[t] = li[t][0+padding:li[t].shape[0]-padding, 0+padding:li[t].shape[1]-padding]\n",
    "\n",
    "\n",
    "        return li, filter, torch.tensor(bias_val)\n",
    "\n",
    "    return Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad46d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 3.1331,  6.5988,  7.8433,  6.5570,  6.3482,  7.2038,  4.6291,  2.1720],\n",
      "        [ 5.4214, 10.5602, 14.8310, 14.6792, 14.4514, 13.5618,  8.8346,  4.7122],\n",
      "        [ 8.6316, 14.2901, 20.4837, 19.3354, 19.2887, 20.2009, 12.1302,  7.1173],\n",
      "        [ 9.2375, 14.3471, 21.5667, 19.9796, 20.3780, 20.4666, 13.0241,  5.8250],\n",
      "        [ 9.0618, 13.9048, 20.3571, 18.3469, 20.3709, 18.7933, 13.9800,  5.8660],\n",
      "        [ 5.4941,  7.8956, 13.7685, 12.0230, 14.0983, 11.3043,  8.5271,  4.8131],\n",
      "        [ 2.6840,  3.4864,  5.6875,  6.1463,  5.8653,  6.0680,  3.1185,  2.1511]]), tensor([[ 2.6149,  4.8761,  7.0615,  6.8501,  6.2363,  6.9232,  5.1690,  3.8666],\n",
      "        [ 4.5600,  7.8935, 12.2280, 13.1924, 12.5405, 12.4767,  7.9589,  5.8672],\n",
      "        [ 7.4190, 11.0547, 17.7375, 18.3442, 17.2785, 16.8891, 12.1377,  6.7916],\n",
      "        [ 7.2754, 11.4281, 18.6729, 18.1914, 19.9586, 18.1694, 12.9631,  6.3629],\n",
      "        [ 7.7623, 10.4667, 17.8200, 15.5612, 18.8985, 16.8510, 12.5185,  6.4444],\n",
      "        [ 5.1930,  6.9634, 12.3248, 10.1281, 11.5424,  9.8449,  7.0174,  5.1266],\n",
      "        [ 2.9152,  3.8819,  5.8431,  4.8152,  5.4753,  6.0645,  4.1036,  2.5660]])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[[ 3.1330602  6.5988426  7.843254   6.5570116  6.3481865  7.203826\n",
      "    4.6291065  2.171977 ]\n",
      "  [ 5.42144   10.56024   14.83104   14.679164  14.451376  13.561782\n",
      "    8.834615   4.712181 ]\n",
      "  [ 8.6315975 14.290077  20.483713  19.335415  19.288692  20.20087\n",
      "   12.130237   7.117304 ]\n",
      "  [ 9.237465  14.347107  21.566723  19.979603  20.378054  20.466566\n",
      "   13.024109   5.825015 ]\n",
      "  [ 9.061816  13.904784  20.357077  18.346859  20.37086   18.79327\n",
      "   13.979981   5.8660173]\n",
      "  [ 5.4941397  7.8956375 13.768488  12.023021  14.098325  11.30426\n",
      "    8.527132   4.8131065]\n",
      "  [ 2.6839864  3.4864376  5.68752    6.1462946  5.8652687  6.068034\n",
      "    3.118521   2.1511188]]\n",
      "\n",
      " [[ 2.6148758  4.8761287  7.0615473  6.85008    6.2363386  6.923171\n",
      "    5.1690326  3.8665597]\n",
      "  [ 4.560035   7.8934784 12.228041  13.192354  12.5404825 12.476693\n",
      "    7.958903   5.8672085]\n",
      "  [ 7.4189863 11.054683  17.73749   18.344158  17.278503  16.889103\n",
      "   12.137719   6.791554 ]\n",
      "  [ 7.2753835 11.42811   18.67287   18.191427  19.95858   18.169436\n",
      "   12.963144   6.362945 ]\n",
      "  [ 7.7623153 10.466654  17.820002  15.5611725 18.898546  16.851007\n",
      "   12.518499   6.4443703]\n",
      "  [ 5.193045   6.963446  12.324777  10.128136  11.542415   9.8449335\n",
      "    7.017377   5.126568 ]\n",
      "  [ 2.9151747  3.8819137  5.8431296  4.815209   5.475343   6.064514\n",
      "    4.103596   2.5660088]]]\n"
     ]
    }
   ],
   "source": [
    "img1 = torch.rand(8, 5, 6)\n",
    "\n",
    "Func = myConvTranspose2d(in_channels=8, out_channels=2, kernel_size=3, stride=1, padding=0, output_padding=0,\n",
    "                         dilation=1, bias=True, padding_mode='zeros')\n",
    "result, kernel, bias_val = Func(img1)\n",
    "torchFunction = torch.nn.ConvTranspose2d(in_channels=8, out_channels=2, kernel_size=3, stride=1, padding=0,\n",
    "                                         output_padding=0, dilation=1, bias=True, padding_mode='zeros')\n",
    "torchFunction.weight.data = kernel\n",
    "torchFunction.bias.data = bias_val\n",
    "\n",
    "print(result);\n",
    "print(end='\\n\\n\\n\\n')\n",
    "print(str(np.array(torchFunction(img1).data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeeb6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.4267, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 1.3524],\n",
      "        [0.8546, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 0.8546],\n",
      "        [0.8546, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 0.8546],\n",
      "        ...,\n",
      "        [0.8546, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 0.8546],\n",
      "        [0.8546, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 0.8546],\n",
      "        [1.0064, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 1.0848]]), tensor([[0.6911, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 1.1186],\n",
      "        [0.2651, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 0.2651],\n",
      "        [0.2651, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 0.2651],\n",
      "        ...,\n",
      "        [0.2651, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 0.2651],\n",
      "        [0.2651, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 0.2651],\n",
      "        [0.4774, 0.2651, 0.2651,  ..., 0.2651, 0.2651, 0.9208]])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[[1.42675    0.85457486 0.85457486 ... 0.85457486 0.85457486 1.3524214 ]\n",
      "  [0.85457486 0.85457486 0.85457486 ... 0.85457486 0.85457486 0.85457486]\n",
      "  [0.85457486 0.85457486 0.85457486 ... 0.85457486 0.85457486 0.85457486]\n",
      "  ...\n",
      "  [0.85457486 0.85457486 0.85457486 ... 0.85457486 0.85457486 0.85457486]\n",
      "  [0.85457486 0.85457486 0.85457486 ... 0.85457486 0.85457486 0.85457486]\n",
      "  [1.0063907  0.85457486 0.85457486 ... 0.85457486 0.85457486 1.0848182 ]]\n",
      "\n",
      " [[0.69111603 0.26511067 0.26511067 ... 0.26511067 0.26511067 1.1186161 ]\n",
      "  [0.26511067 0.26511067 0.26511067 ... 0.26511067 0.26511067 0.26511067]\n",
      "  [0.26511067 0.26511067 0.26511067 ... 0.26511067 0.26511067 0.26511067]\n",
      "  ...\n",
      "  [0.26511067 0.26511067 0.26511067 ... 0.26511067 0.26511067 0.26511067]\n",
      "  [0.26511067 0.26511067 0.26511067 ... 0.26511067 0.26511067 0.26511067]\n",
      "  [0.4774335  0.26511067 0.26511067 ... 0.26511067 0.26511067 0.9208309 ]]]\n"
     ]
    }
   ],
   "source": [
    "img2 = torch.rand(3, 28, 28)\n",
    "\n",
    "Func2 = myConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=10, padding=0,\n",
    "                          output_padding=0, dilation=3, bias=True, padding_mode='zeros')\n",
    "result, kernel, bias_val = Func2(img2)\n",
    "torchFunction2 = torch.nn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=10, padding=0,\n",
    "                                          output_padding=0, dilation=3, bias=True, padding_mode='zeros')\n",
    "torchFunction2.weight.data = kernel\n",
    "torchFunction2.bias.data = bias_val\n",
    "\n",
    "print(result);\n",
    "print(end='\\n\\n\\n\\n')\n",
    "print(str(np.array(torchFunction2(img2).data)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d7a6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.4245, 0.8285, 0.6493, 0.8031, 1.0417, 1.2434, 1.4128, 0.9784, 0.8634,\n",
      "         1.2265],\n",
      "        [1.7626, 1.6280, 1.6336, 1.3068, 1.5904, 1.4299, 1.3380, 1.2563, 1.3884,\n",
      "         1.3867],\n",
      "        [1.0492, 1.1578, 1.8666, 0.6443, 1.6881, 1.8220, 0.9544, 0.7994, 1.4644,\n",
      "         0.6034],\n",
      "        [1.5859, 1.3923, 1.0404, 1.5442, 1.4546, 1.4025, 1.1356, 1.0652, 0.7482,\n",
      "         1.0958],\n",
      "        [1.7674, 1.1218, 1.2143, 1.1004, 2.0716, 2.3750, 1.8421, 1.7587, 1.9214,\n",
      "         1.6972],\n",
      "        [0.7710, 1.4819, 1.8958, 0.3642, 1.6712, 2.2249, 1.1251, 1.4839, 1.8148,\n",
      "         1.1210],\n",
      "        [1.8679, 1.0996, 1.8168, 1.6252, 1.7639, 1.4604, 2.0899, 1.5154, 1.1752,\n",
      "         1.4218],\n",
      "        [1.0385, 1.0927, 1.3357, 1.1244, 1.6068, 1.9509, 1.6546, 1.6209, 1.2246,\n",
      "         1.1433],\n",
      "        [0.3844, 0.8349, 1.5216, 0.3429, 1.4247, 1.9245, 0.8721, 1.4698, 1.9388,\n",
      "         0.6730],\n",
      "        [1.0597, 0.9510, 1.0465, 1.4904, 1.3996, 1.4255, 1.7996, 1.4678, 1.2180,\n",
      "         1.0920]])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[[1.4244859  0.82853246 0.64926565 0.8031102  1.0416505  1.2434001\n",
      "   1.4128261  0.97841644 0.8634349  1.226475  ]\n",
      "  [1.7625601  1.6280322  1.6336238  1.3068349  1.5904028  1.4299412\n",
      "   1.3379774  1.2562578  1.3883843  1.3867276 ]\n",
      "  [1.0492023  1.1577594  1.866642   0.6442574  1.6881359  1.8219638\n",
      "   0.9544335  0.79941136 1.4644096  0.6033772 ]\n",
      "  [1.5859149  1.3922596  1.0404348  1.5441909  1.4545515  1.4025037\n",
      "   1.1355896  1.0651727  0.7482227  1.0957859 ]\n",
      "  [1.7674417  1.1218324  1.2142844  1.100368   2.0715697  2.3749537\n",
      "   1.8420894  1.758662   1.9213843  1.697156  ]\n",
      "  [0.7710208  1.481926   1.895782   0.36419395 1.6712146  2.2249165\n",
      "   1.1251171  1.4839356  1.8148386  1.1209528 ]\n",
      "  [1.8679256  1.0996068  1.8167737  1.6251924  1.7638905  1.4603691\n",
      "   2.089896   1.5153892  1.1751986  1.4217937 ]\n",
      "  [1.0385461  1.0926793  1.3356907  1.1243501  1.6067905  1.9508872\n",
      "   1.6545546  1.6208665  1.22457    1.1432836 ]\n",
      "  [0.38436276 0.83485216 1.5216186  0.34292206 1.4246807  1.9244578\n",
      "   0.8721412  1.4698405  1.9387665  0.672957  ]\n",
      "  [1.0597246  0.9510223  1.0465469  1.4904473  1.399585   1.425462\n",
      "   1.7996025  1.4677715  1.2180104  1.0920358 ]]]\n"
     ]
    }
   ],
   "source": [
    "img3 = torch.rand(5, 6, 6)\n",
    "\n",
    "Func3 = myConvTranspose2d(in_channels=5, out_channels=1, kernel_size=3, stride=3, padding=5,\n",
    "                          output_padding=2, dilation=1, bias=True, padding_mode='zeros')\n",
    "result, kernel, bias_val = Func3(img3)\n",
    "torchFunction3 = torch.nn.ConvTranspose2d(in_channels=5, out_channels=1, kernel_size=3, stride=3,\n",
    "                                          padding=5, output_padding=2, dilation=1, bias=True, padding_mode='zeros')\n",
    "torchFunction3.weight.data = kernel\n",
    "torchFunction3.bias.data = bias_val\n",
    "\n",
    "print(result);\n",
    "print(end='\\n\\n\\n\\n')\n",
    "print(str(np.array(torchFunction3(img3).data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
